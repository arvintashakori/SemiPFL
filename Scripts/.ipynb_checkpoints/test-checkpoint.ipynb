{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from models import HN, Autoencoder, BASEModel, MODEL\n",
    "from collections import OrderedDict, defaultdict\n",
    "from utils import get_default_device, set_seed, f1_loss\n",
    "from node import Clients\n",
    "\n",
    "import os\n",
    "from tqdm import trange\n",
    "import pandas as pd\n",
    "import random\n",
    "\n",
    "import torch\n",
    "from torch import nn\n",
    "import torch.utils.data\n",
    "from torchvision import transforms\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "class parameters:\n",
    "    def __init__(self):\n",
    "        self.seed = 0\n",
    "        self.labels_list = ['JOG', 'JUM', 'STD', 'WAL']  # list of activities\n",
    "        self.outputdim = len(self.labels_list)\n",
    "        self.data_address = r\"C:\\Users\\walke\\Documents\\GitHub\\SemiPFL_Wenwen\\MobiNpy_4_Act\\user\"  # data adress\n",
    "#         self.data_address = os.path.abspath(os.path.join(\n",
    "#             os.getcwd(), os.pardir)) + \"/Datasets/MobiNpy_4_Act/\"  # data adress\n",
    "        self.trial_number = 1  # which trial we use for this test\n",
    "        self.label_ratio = 0.10  # ratio of labeled data\n",
    "        self.eval_ratio = 0.30  # ratio of eval data\n",
    "        self.number_of_client = 1  # total number of clients\n",
    "        self.server_ID = [0]  # server ID\n",
    "        self.batch_size = 128  # training batch size\n",
    "        self.window_size = 30  # window size (for our case 30)\n",
    "        self.width = 9  # data dimension (AX, AY, AZ) (GX, GY, GZ) (MX, MY, MZ)\n",
    "        self.n_kernels = 16  # number of kernels for hypernetwork\n",
    "        self.total_number_of_clients = 59  # total number of subjects (client + server)\n",
    "        self.learning_rate = 1e-4  # learning rate for optimizer\n",
    "        self.steps = 10  # total number of epochs\n",
    "        self.inner_step_for_AE = 5  # number of epochs to fine tunne the Autoencoder\n",
    "        self.inner_step_server_finetune = 5  # number of steps in the server side to finetune\n",
    "        self.inner_step_for_model = 5  # number of steps that server fine tune its hn and user embedding parameters\n",
    "        self.model_loop = False  # feedback loop for user model\n",
    "        self.inner_step_for_client = 5  # number of steps that user fine tune its model\n",
    "        self.inner_lr = 1e-4  # user learning rate\n",
    "        self.inner_wd = 5e-5  # weight decay\n",
    "        self.inout_channels = 1  # number of channels\n",
    "        self.hidden = 16  # Autoencoder layer 2 parameters\n",
    "        self.n_kernels_enc = 3  # autoencoder encoder kernel size\n",
    "        self.hidden_dim_for_HN = 100  # hidden dimension for hypernetwork\n",
    "        self.n_kernels_dec = 3  # autoencoder decoder kernel size\n",
    "        self.latent_rep = 4  # latent reperesentation size\n",
    "        self.n_hidden_HN = 100  # number of hidden layers in hypernetworks\n",
    "        self.stride_value = 1  # stride value for autoencoder\n",
    "        self.padding_value = 1  # padding value for autoencoder\n",
    "        self.model_hidden_layer =128  # final model hidden layer size\n",
    "        self.spec_norm = False  # True if you want to use spectral norm\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "params = parameters()\n",
    "set_seed(params.seed)\n",
    "transform = transforms.Compose([transforms.ToTensor(), transforms.Normalize((0.5,), (0.5,))])\n",
    "device = get_default_device()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = MODEL(inout_channels=params.inout_channels, hidden=params.hidden, n_kernels_enc=params.n_kernels_enc,\n",
    "                     n_kernels_dec=params.n_kernels_dec, latent_rep=params.latent_rep, stride_value=params.stride_value,\n",
    "                     padding_value=params.padding_value, latent_rep_fc= 4 * 3 * 6, out_dim=params.outputdim, hidden_layer=params.model_hidden_layer)  # initializing the autoencoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "MODEL(\n",
       "  (conv1): Conv2d(1, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "  (conv2): Conv2d(16, 4, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "  (pool): MaxPool2d(kernel_size=(3, 5), stride=(3, 5), padding=0, dilation=1, ceil_mode=False)\n",
       "  (batch1): BatchNorm1d(72, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (fc1): Linear(in_features=72, out_features=128, bias=True)\n",
       "  (batch2): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (fc2): Linear(in_features=128, out_features=4, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    " criteria_model = torch.nn.NLLLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "inner_optim = torch.optim.Adam(model.parameters(), lr=params.inner_lr, weight_decay=params.inner_wd)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "nodes = Clients(address=params.data_address,\n",
    "                    trial_number=params.trial_number,\n",
    "                    label_ratio=params.label_ratio,\n",
    "                    server_ID=params.server_ID,\n",
    "                    eval_ratio=params.eval_ratio,\n",
    "                    window_size=params.window_size,\n",
    "                    width=params.width,\n",
    "                    transform=transform,\n",
    "                    num_user=params.total_number_of_clients)\n",
    "# dataloaders\n",
    "client_loader = []\n",
    "client_labeled_loaders = []\n",
    "eval_loader = []\n",
    "server_loaders = torch.utils.data.DataLoader(\n",
    "    nodes.server_loaders, batch_size=params.batch_size, shuffle=True)\n",
    "\n",
    "for i in range(params.number_of_client):\n",
    "    client_loader.append(torch.utils.data.DataLoader(\n",
    "        nodes.client_loaders[i], batch_size=params.batch_size, shuffle=True))\n",
    "    client_labeled_loaders.append(torch.utils.data.DataLoader(\n",
    "        nodes.client_labeled_loaders[i], batch_size=params.batch_size, shuffle=True))\n",
    "    eval_loader.append(torch.utils.data.DataLoader(\n",
    "        nodes.eval_data[i], batch_size=params.batch_size, shuffle=True))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "modelfine epoch [1/5], loss:0.6213\n",
      "modelfine epoch [2/5], loss:0.5873\n",
      "modelfine epoch [3/5], loss:0.5348\n",
      "modelfine epoch [4/5], loss:0.7020\n",
      "modelfine epoch [5/5], loss:0.6667\n"
     ]
    }
   ],
   "source": [
    "model.train()\n",
    "for i in range(params.inner_step_for_model):\n",
    "    inner_optim.zero_grad()\n",
    "    for sensor_values, activity in client_labeled_loaders[0]:\n",
    "        predicted_activity = model(sensor_values.to(device).float())\n",
    "        loss = criteria_model(predicted_activity, activity.to(device))\n",
    "        loss.backward()\n",
    "        inner_optim.step()\n",
    "    print('modelfine epoch [{}/{}], loss:{:.4f}'.format(i + 1, params.inner_step_for_model, loss.data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss: 0.46900937702734147\n",
      "f1: 0.8036385009132163\n"
     ]
    }
   ],
   "source": [
    "with torch.no_grad():\n",
    "    model.eval()\n",
    "    prvs_loss_server_model = 0\n",
    "    f1_score_server = 0\n",
    "    for sensor_values, activity in eval_loader[0]:\n",
    "        predicted_activity = model(sensor_values.to(device).float())\n",
    "        prvs_loss_server_model += criteria_model(predicted_activity, activity.to(device)).item() * sensor_values.size(0)\n",
    "        f1_score_server += f1_loss(activity, predicted_activity) * sensor_values.size(0)\n",
    "    prvs_loss_server_model /= len(eval_loader[0].dataset)\n",
    "    f1_score_server /= len(eval_loader[0].dataset)\n",
    "\n",
    "print(\"loss: \" + str(prvs_loss_server_model))\n",
    "print(\"f1: \" + str(f1_score_server))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Autoencoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn.functional as F\n",
    "class Autoencoder1(nn.Module):\n",
    "    def __init__(self, inout_channels=1, hidden=16, n_kernels_enc=3,\n",
    "                 n_kernels_dec=3, latent_rep=4, stride_value=1, padding_value=1):\n",
    "        super(Autoencoder1, self).__init__()\n",
    "        # Encoder\n",
    "        self.conv1 = nn.Conv2d(inout_channels, hidden,\n",
    "                               n_kernels_enc, padding=padding_value)  # hidden*9*30\n",
    "        self.conv2 = nn.Conv2d(\n",
    "            hidden, latent_rep, n_kernels_enc, padding=padding_value)  # latent_rep*9*30\n",
    "        self.pool = nn.MaxPool2d((3, 5), stride=(3, 5))  # latent_rep*3*6\n",
    "        # self.pool = nn.MaxPool2d(3, stride=2)\n",
    "        # self.conv3 = nn.Conv2d(8, latent_rep, n_kernels_enc, padding=padding_value)\n",
    "        # batch norms\n",
    "        self.batchNorm1 = nn.BatchNorm2d(16)\n",
    "        self.batchNorm2 = nn.BatchNorm2d(4)\n",
    "        # Decoder\n",
    "\n",
    "        #        self.t_conv1 = nn.ConvTranspose2d(\n",
    "        #            latent_rep, hidden, kernel_size=(n_kernels_dec, n_kernels_dec), stride=stride_value, padding=padding_value)\n",
    "        #        self.t_conv2 = nn.ConvTranspose2d(\n",
    "        #            hidden, inout_channels, kernel_size=(n_kernels_dec, n_kernels_dec), stride=stride_value,\n",
    "        #            padding=padding_value)\n",
    "\n",
    "        self.t_conv1 = nn.ConvTranspose2d(\n",
    "            latent_rep, hidden, kernel_size=(n_kernels_dec, n_kernels_dec), stride=stride_value, padding=padding_value)\n",
    "        # self.t_conv2 = nn.ConvTranspose2d(\n",
    "        #     hidden, latent_rep, kernel_size=(n_kernels_dec, n_kernels_dec), stride=stride_value, padding=padding_value)\n",
    "        self.t_conv2 = nn.ConvTranspose2d(\n",
    "            hidden, inout_channels, kernel_size=(3, 5), stride=(3, 5), padding=0)\n",
    "        print('')\n",
    "\n",
    "    def encoder(self, x):\n",
    "        #print(\"input: \" + str(x.shape))\n",
    "        z = F.relu(self.batchNorm1(self.conv1(x)))\n",
    "        #print(\"conv1: \" + str(z.shape))\n",
    "        z = self.pool(F.relu(self.batchNorm2(self.conv2(z))))\n",
    "        #print(\"conv2: \" + str(z.shape))\n",
    "        # z = F.relu(self.conv3(z))\n",
    "        return z\n",
    "\n",
    "    def decoder(self, x):\n",
    "        z = F.relu(self.t_conv1(x))\n",
    "        #print (\"t_conv1: \" + str(z.shape))\n",
    "        z = torch.tanh(self.t_conv2(z))\n",
    "        #print(\"t_conv2: \" + str(z.shape))\n",
    "        # z = (self.t_conv3(z))\n",
    "        #print(\"t_conv3: \" + str(z.shape))\n",
    "        # z = t.sigmoid(self.t_conv3(z))\n",
    "        return z\n",
    "    \n",
    "    def forward(self, input):\n",
    "        latent_representation = self.encoder(input)\n",
    "        output = self.decoder(latent_representation)\n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "AE = Autoencoder1(inout_channels=params.inout_channels, hidden=params.hidden, n_kernels_enc=params.n_kernels_enc,\n",
    "                     n_kernels_dec=params.n_kernels_dec, latent_rep=params.latent_rep, stride_value=params.stride_value,\n",
    "                     padding_value=params.padding_value)  # initializing the autoencoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Autoencoder1(\n",
       "  (conv1): Conv2d(1, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "  (conv2): Conv2d(16, 4, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "  (pool): MaxPool2d(kernel_size=(3, 5), stride=(3, 5), padding=0, dilation=1, ceil_mode=False)\n",
       "  (batchNorm1): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (batchNorm2): BatchNorm2d(4, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (t_conv1): ConvTranspose2d(4, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "  (t_conv2): ConvTranspose2d(16, 1, kernel_size=(3, 5), stride=(3, 5))\n",
       ")"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "AE.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = torch.optim.Adam(params=AE.parameters(), lr=params.learning_rate)\n",
    "criteria_AE = torch.nn.MSELoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "running_mean should contain 4 elements not 16",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-14-9a7323b15e56>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      3\u001b[0m     \u001b[0minner_optim\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mzero_grad\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m     \u001b[1;32mfor\u001b[0m \u001b[0msensor_values\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mactivity\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mclient_labeled_loaders\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 5\u001b[1;33m         \u001b[0mpredicted_activity\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mAE\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msensor_values\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mto\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfloat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      6\u001b[0m         \u001b[0mloss\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcriteria_AE\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpredicted_activity\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfloat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msensor_values\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mto\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfloat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      7\u001b[0m         \u001b[0mloss\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\walke\\appdata\\local\\programs\\python\\python38\\lib\\site-packages\\torch\\nn\\modules\\module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m   1049\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[0;32m   1050\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[1;32m-> 1051\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1052\u001b[0m         \u001b[1;31m# Do not call functions when jit is used\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1053\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-10-2acf9e8bbac6>\u001b[0m in \u001b[0;36mforward\u001b[1;34m(self, input)\u001b[0m\n\u001b[0;32m     51\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     52\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minput\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 53\u001b[1;33m         \u001b[0mlatent_representation\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mencoder\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     54\u001b[0m         \u001b[0moutput\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdecoder\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlatent_representation\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     55\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0moutput\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-10-2acf9e8bbac6>\u001b[0m in \u001b[0;36mencoder\u001b[1;34m(self, x)\u001b[0m\n\u001b[0;32m     35\u001b[0m         \u001b[0mz\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mF\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrelu\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbatchNorm1\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mconv1\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     36\u001b[0m         \u001b[1;31m#print(\"conv1: \" + str(z.shape))\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 37\u001b[1;33m         \u001b[0mz\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpool\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mF\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrelu\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbatchNorm1\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mconv2\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mz\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     38\u001b[0m         \u001b[1;31m#print(\"conv2: \" + str(z.shape))\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     39\u001b[0m         \u001b[1;31m# z = F.relu(self.conv3(z))\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\walke\\appdata\\local\\programs\\python\\python38\\lib\\site-packages\\torch\\nn\\modules\\module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m   1049\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[0;32m   1050\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[1;32m-> 1051\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1052\u001b[0m         \u001b[1;31m# Do not call functions when jit is used\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1053\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\walke\\appdata\\local\\programs\\python\\python38\\lib\\site-packages\\torch\\nn\\modules\\batchnorm.py\u001b[0m in \u001b[0;36mforward\u001b[1;34m(self, input)\u001b[0m\n\u001b[0;32m    165\u001b[0m         \u001b[0mused\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mnormalization\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0me\u001b[0m\u001b[1;33m.\u001b[0m \u001b[1;32min\u001b[0m \u001b[0meval\u001b[0m \u001b[0mmode\u001b[0m \u001b[0mwhen\u001b[0m \u001b[0mbuffers\u001b[0m \u001b[0mare\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    166\u001b[0m         \"\"\"\n\u001b[1;32m--> 167\u001b[1;33m         return F.batch_norm(\n\u001b[0m\u001b[0;32m    168\u001b[0m             \u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    169\u001b[0m             \u001b[1;31m# If buffers are not to be tracked, ensure that they won't be updated\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\walke\\appdata\\local\\programs\\python\\python38\\lib\\site-packages\\torch\\nn\\functional.py\u001b[0m in \u001b[0;36mbatch_norm\u001b[1;34m(input, running_mean, running_var, weight, bias, training, momentum, eps)\u001b[0m\n\u001b[0;32m   2279\u001b[0m         \u001b[0m_verify_batch_size\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msize\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2280\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 2281\u001b[1;33m     return torch.batch_norm(\n\u001b[0m\u001b[0;32m   2282\u001b[0m         \u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mweight\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbias\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mrunning_mean\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mrunning_var\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtraining\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmomentum\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0meps\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbackends\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcudnn\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0menabled\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2283\u001b[0m     )\n",
      "\u001b[1;31mRuntimeError\u001b[0m: running_mean should contain 4 elements not 16"
     ]
    }
   ],
   "source": [
    "AE.train()\n",
    "for i in range(params.inner_step_for_model):\n",
    "    inner_optim.zero_grad()\n",
    "    for sensor_values, activity in client_labeled_loaders[0]:\n",
    "        predicted_activity = AE(sensor_values.to(device).float())\n",
    "        loss = criteria_AE(predicted_activity.float(), sensor_values.to(device).float())\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "    print('modelfine epoch [{}/{}], loss:{:.4f}'.format(i + 1, params.inner_step_for_model, loss.data))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.6rc1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
