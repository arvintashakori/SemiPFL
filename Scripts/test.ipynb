{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "from models import HN, Autoencoder, BASEModel, MODEL\n",
    "from collections import OrderedDict, defaultdict\n",
    "from utils import get_default_device, set_seed, f1_loss\n",
    "from node import Clients\n",
    "\n",
    "import os\n",
    "from tqdm import trange\n",
    "import pandas as pd\n",
    "import random\n",
    "\n",
    "import torch\n",
    "from torch import nn\n",
    "import torch.utils.data\n",
    "from torchvision import transforms\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "class parameters:\n",
    "    def __init__(self):\n",
    "        self.seed = 0\n",
    "        self.labels_list = ['JOG', 'JUM', 'STD', 'WAL']  # list of activities\n",
    "        self.outputdim = len(self.labels_list)\n",
    "        self.data_address = os.path.abspath(os.path.join(os.getcwd(), os.pardir)) + \"\\\\Datasets\\\\MobiNpy_4_Act\\\\\"  # data adress\n",
    "#         self.data_address = os.path.abspath(os.path.join(\n",
    "#             os.getcwd(), os.pardir)) + \"/Datasets/MobiNpy_4_Act/\"  # data adress\n",
    "        self.trial_number = 1  # which trial we use for this test\n",
    "        self.label_ratio = 0.10  # ratio of labeled data\n",
    "        self.eval_ratio = 0.30  # ratio of eval data\n",
    "        self.number_of_client = 1  # total number of clients\n",
    "        self.server_ID = [0]  # server ID\n",
    "        self.batch_size = 128  # training batch size\n",
    "        self.window_size = 30  # window size (for our case 30)\n",
    "        self.width = 9  # data dimension (AX, AY, AZ) (GX, GY, GZ) (MX, MY, MZ)\n",
    "        self.n_kernels = 16  # number of kernels for hypernetwork\n",
    "        self.total_number_of_clients = 59  # total number of subjects (client + server)\n",
    "        self.learning_rate = 0.001  # learning rate for optimizer\n",
    "        self.steps = 10  # total number of epochs\n",
    "        self.inner_step_for_AE = 5  # number of epochs to fine tunne the Autoencoder\n",
    "        self.inner_step_server_finetune = 5  # number of steps in the server side to finetune\n",
    "        self.inner_step_for_model = 5  # number of steps that server fine tune its hn and user embedding parameters\n",
    "        self.model_loop = False  # feedback loop for user model\n",
    "        self.inner_step_for_client = 5  # number of steps that user fine tune its model\n",
    "        self.inner_lr = 0.001  # user learning rate\n",
    "        self.inner_wd = 5e-5  # weight decay\n",
    "        self.inout_channels = 1  # number of channels\n",
    "        self.hidden = 16  # Autoencoder layer 2 parameters\n",
    "        self.n_kernels_enc = 3  # autoencoder encoder kernel size\n",
    "        self.hidden_dim_for_HN = 100  # hidden dimension for hypernetwork\n",
    "        self.n_kernels_dec = 3  # autoencoder decoder kernel size\n",
    "        self.latent_rep = 4  # latent reperesentation size\n",
    "        self.n_hidden_HN = 100  # number of hidden layers in hypernetworks\n",
    "        self.stride_value = 1  # stride value for autoencoder\n",
    "        self.padding_value = 1  # padding value for autoencoder\n",
    "        self.model_hidden_layer =128  # final model hidden layer size\n",
    "        self.spec_norm = False  # True if you want to use spectral norm\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "params = parameters()\n",
    "set_seed(params.seed)\n",
    "transform = transforms.Compose([transforms.ToTensor(), transforms.Normalize((0.5,), (0.5,))])\n",
    "device = get_default_device()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = MODEL(inout_channels=params.inout_channels, hidden=params.hidden, n_kernels_enc=params.n_kernels_enc,\n",
    "                     n_kernels_dec=params.n_kernels_dec, latent_rep=params.latent_rep, stride_value=params.stride_value,\n",
    "                     padding_value=params.padding_value, latent_rep_fc= 4 * 3 * 6, out_dim=params.outputdim, hidden_layer=params.model_hidden_layer)  # initializing the autoencoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "MODEL(\n",
       "  (conv1): Conv2d(1, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "  (conv2): Conv2d(16, 4, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "  (pool): MaxPool2d(kernel_size=(3, 5), stride=(3, 5), padding=0, dilation=1, ceil_mode=False)\n",
       "  (batch1): BatchNorm1d(72, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (fc1): Linear(in_features=72, out_features=128, bias=True)\n",
       "  (batch2): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (fc2): Linear(in_features=128, out_features=4, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    " criteria_model = torch.nn.NLLLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "inner_optim = torch.optim.Adam(model.parameters(), lr=params.inner_lr, weight_decay=params.inner_wd)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "nodes = Clients(address=params.data_address,\n",
    "                    trial_number=params.trial_number,\n",
    "                    label_ratio=params.label_ratio,\n",
    "                    server_ID=params.server_ID,\n",
    "                    eval_ratio=params.eval_ratio,\n",
    "                    window_size=params.window_size,\n",
    "                    width=params.width,\n",
    "                    transform=transform,\n",
    "                    num_user=params.total_number_of_clients)\n",
    "# dataloaders\n",
    "client_loader = []\n",
    "client_labeled_loaders = []\n",
    "eval_loader = []\n",
    "server_loaders = torch.utils.data.DataLoader(\n",
    "    nodes.server_loaders, batch_size=params.batch_size, shuffle=True)\n",
    "\n",
    "for i in range(params.number_of_client):\n",
    "    client_loader.append(torch.utils.data.DataLoader(\n",
    "        nodes.client_loaders[i], batch_size=params.batch_size, shuffle=True))\n",
    "    client_labeled_loaders.append(torch.utils.data.DataLoader(\n",
    "        nodes.client_labeled_loaders[i], batch_size=params.batch_size, shuffle=True))\n",
    "    eval_loader.append(torch.utils.data.DataLoader(\n",
    "        nodes.eval_data[i], batch_size=params.batch_size, shuffle=True))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "modelfine epoch [1/5], loss:0.1649\n",
      "modelfine epoch [2/5], loss:0.2908\n",
      "modelfine epoch [3/5], loss:0.0979\n",
      "modelfine epoch [4/5], loss:0.4567\n",
      "modelfine epoch [5/5], loss:0.1631\n",
      "modelfine epoch [6/5], loss:0.2107\n",
      "modelfine epoch [7/5], loss:0.2161\n",
      "modelfine epoch [8/5], loss:0.0094\n",
      "modelfine epoch [9/5], loss:0.0112\n",
      "modelfine epoch [10/5], loss:0.0006\n",
      "modelfine epoch [11/5], loss:0.0383\n",
      "modelfine epoch [12/5], loss:0.0002\n",
      "modelfine epoch [13/5], loss:0.0082\n",
      "modelfine epoch [14/5], loss:0.1261\n",
      "modelfine epoch [15/5], loss:0.1513\n",
      "modelfine epoch [16/5], loss:0.0272\n",
      "modelfine epoch [17/5], loss:0.0431\n",
      "modelfine epoch [18/5], loss:0.1837\n",
      "modelfine epoch [19/5], loss:0.0030\n",
      "modelfine epoch [20/5], loss:0.0070\n"
     ]
    }
   ],
   "source": [
    "model.train()\n",
    "for i in range(20):\n",
    "    inner_optim.zero_grad()\n",
    "    for sensor_values, activity in client_labeled_loaders[0]:\n",
    "        predicted_activity = model(sensor_values.to(device).float())\n",
    "        loss = criteria_model(predicted_activity, activity.to(device))\n",
    "        loss.backward()\n",
    "        inner_optim.step()\n",
    "    print('modelfine epoch [{}/{}], loss:{:.4f}'.format(i + 1, params.inner_step_for_model, loss.data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss: 0.006253385489769368\n",
      "f1: 0.998384355370485\n"
     ]
    }
   ],
   "source": [
    "with torch.no_grad():\n",
    "    model.eval()\n",
    "    prvs_loss_server_model = 0\n",
    "    f1_score_server = 0\n",
    "    for sensor_values, activity in eval_loader[0]:\n",
    "        predicted_activity = model(sensor_values.to(device).float())\n",
    "        prvs_loss_server_model += criteria_model(predicted_activity, activity.to(device)).item() * sensor_values.size(0)\n",
    "        f1_score_server += f1_loss(activity, predicted_activity) * sensor_values.size(0)\n",
    "    prvs_loss_server_model /= len(eval_loader[0].dataset)\n",
    "    f1_score_server /= len(eval_loader[0].dataset)\n",
    "\n",
    "print(\"loss: \" + str(prvs_loss_server_model))\n",
    "print(\"f1: \" + str(f1_score_server))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Autoencoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn.functional as F\n",
    "class autoencoder(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(autoencoder, self).__init__()\n",
    "        self.encoder = nn.Sequential(\n",
    "            nn.Linear(270, 256),\n",
    "            nn.ReLU(True),\n",
    "            nn.BatchNorm1d(256),\n",
    "            nn.Linear(256, 128),\n",
    "            nn.ReLU(True),\n",
    "            nn.Linear(128, 64))\n",
    "        self.decoder = nn.Sequential(\n",
    "            nn.Linear(64, 128),\n",
    "            nn.ReLU(True),\n",
    "            nn.Linear(128, 256),\n",
    "            nn.ReLU(True),\n",
    "            nn.BatchNorm1d(256),\n",
    "            nn.Linear(256, 270),\n",
    "            nn.Tanh())\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.encoder(x)\n",
    "        x = self.decoder(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "autoencoder(\n",
       "  (encoder): Sequential(\n",
       "    (0): Linear(in_features=270, out_features=256, bias=True)\n",
       "    (1): ReLU(inplace=True)\n",
       "    (2): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (3): Linear(in_features=256, out_features=128, bias=True)\n",
       "    (4): ReLU(inplace=True)\n",
       "    (5): Linear(in_features=128, out_features=64, bias=True)\n",
       "  )\n",
       "  (decoder): Sequential(\n",
       "    (0): Linear(in_features=64, out_features=128, bias=True)\n",
       "    (1): ReLU(inplace=True)\n",
       "    (2): Linear(in_features=128, out_features=256, bias=True)\n",
       "    (3): ReLU(inplace=True)\n",
       "    (4): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (5): Linear(in_features=256, out_features=270, bias=True)\n",
       "    (6): Tanh()\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "AE = autoencoder()\n",
    "AE.to(device)\n",
    "criterion = nn.MSELoss()\n",
    "optimizer = torch.optim.Adam(AE.parameters(), lr=1e-3, weight_decay=1e-5)\n",
    "AE.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AE epoch [0/100], loss:0.5809\n",
      "AE epoch [10/100], loss:0.4224\n",
      "AE epoch [20/100], loss:0.3664\n",
      "AE epoch [30/100], loss:0.2795\n",
      "AE epoch [40/100], loss:0.2123\n",
      "AE epoch [50/100], loss:0.2838\n",
      "AE epoch [60/100], loss:0.2440\n",
      "AE epoch [70/100], loss:0.3066\n",
      "AE epoch [80/100], loss:0.2530\n",
      "AE epoch [90/100], loss:0.2740\n"
     ]
    }
   ],
   "source": [
    "from torch.autograd import Variable\n",
    "\n",
    "for epoch in range(100):\n",
    "    for sensor, _ in client_loader[0]:\n",
    "        sensor = sensor.view(sensor.size(0), -1)\n",
    "        sensor = Variable(sensor).to(device)\n",
    "        # ===================forward=====================\n",
    "        output = AE(sensor.float())\n",
    "        loss = criterion(output, sensor.float())\n",
    "        # ===================backward====================\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "    # ===================log========================\n",
    "    if epoch % 10 == 0:\n",
    "        print('AE epoch [{}/{}], loss:{:.4f}'.format(epoch, 100, loss.data))\n",
    "   \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn.functional as F\n",
    "class BASEModel(nn.Module):\n",
    "    def __init__(self): \n",
    "        super(BASEModel, self).__init__()\n",
    "\n",
    "        self.batch1 = nn.BatchNorm1d(64)\n",
    "        self.batch2 = nn.BatchNorm1d(16)\n",
    "        self.fc1 = nn.Linear(64, 16)\n",
    "        self.fc2 = nn.Linear(16, 4)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.batch1(x)\n",
    "        x = self.batch2(F.relu(self.fc1(x)))\n",
    "        x = F.log_softmax(self.fc2(x), dim=1)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "BASEModel(\n",
       "  (batch1): BatchNorm1d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (batch2): BatchNorm1d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (fc1): Linear(in_features=64, out_features=16, bias=True)\n",
       "  (fc2): Linear(in_features=16, out_features=4, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = BASEModel()\n",
    "model.to(device)\n",
    "criterion = torch.nn.NLLLoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=1e-3, weight_decay=1e-5)\n",
    "AE.eval\n",
    "model.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AE epoch [0/100], loss:0.4911\n",
      "AE epoch [10/100], loss:0.0496\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(50):\n",
    "    for sensor, activity in client_labeled_loaders[0]:\n",
    "        sensor = sensor.view(sensor.size(0), -1)\n",
    "        encoded = AE.encoder(sensor.to(device).float())\n",
    "        predicted = model(encoded)\n",
    "        # ===================forward=====================\n",
    "        loss = criterion(predicted, activity.to(device))\n",
    "        # ===================backward====================\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "    # ===================log========================\n",
    "    if epoch % 10 == 0:\n",
    "        print('AE epoch [{}/{}], loss:{:.4f}'.format(epoch, 100, loss.data))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss: 0.002365735809062116\n",
      "f1: 0.999118982623249\n"
     ]
    }
   ],
   "source": [
    "with torch.no_grad():\n",
    "    model.eval()\n",
    "    prvs_loss_server_model = 0\n",
    "    f1_score_server = 0\n",
    "    for sensor_values, activity in eval_loader[0]:\n",
    "        sensor_values = sensor_values.view(sensor_values.size(0), -1)\n",
    "        encoded_sensor = AE.encoder(sensor_values.to(device).float())\n",
    "        predicted_activity = model(encoded_sensor)\n",
    "        prvs_loss_server_model += criteria_model(predicted_activity, activity.to(device)).item() * sensor_values.size(0)\n",
    "        f1_score_server += f1_loss(activity, predicted_activity) * sensor_values.size(0)\n",
    "    prvs_loss_server_model /= len(eval_loader[0].dataset)\n",
    "    f1_score_server /= len(eval_loader[0].dataset)\n",
    "\n",
    "print(\"loss: \" + str(prvs_loss_server_model))\n",
    "print(\"f1: \" + str(f1_score_server))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "import torch.nn.functional as F\n",
    "class Autoencoder1(nn.Module):\n",
    "    def __init__(self, inout_channels=1, hidden=16, n_kernels_enc=3,\n",
    "                 n_kernels_dec=3, latent_rep=4, stride_value=1, padding_value=1):\n",
    "        super(Autoencoder1, self).__init__()\n",
    "        # Encoder\n",
    "        self.conv1 = nn.Conv2d(inout_channels, hidden,\n",
    "                               n_kernels_enc, padding=padding_value)  # hidden*9*30\n",
    "        self.conv2 = nn.Conv2d(\n",
    "            hidden, latent_rep, n_kernels_enc, padding=padding_value)  # latent_rep*9*30\n",
    "        self.pool = nn.MaxPool2d((3, 5), stride=(3, 5))  # latent_rep*3*6\n",
    "        # self.pool = nn.MaxPool2d(3, stride=2)\n",
    "        # self.conv3 = nn.Conv2d(8, latent_rep, n_kernels_enc, padding=padding_value)\n",
    "        # batch norms\n",
    "        self.batchNorm1 = nn.BatchNorm2d(16)\n",
    "        self.batchNorm2 = nn.BatchNorm2d(4)\n",
    "        # Decoder\n",
    "\n",
    "        #        self.t_conv1 = nn.ConvTranspose2d(\n",
    "        #            latent_rep, hidden, kernel_size=(n_kernels_dec, n_kernels_dec), stride=stride_value, padding=padding_value)\n",
    "        #        self.t_conv2 = nn.ConvTranspose2d(\n",
    "        #            hidden, inout_channels, kernel_size=(n_kernels_dec, n_kernels_dec), stride=stride_value,\n",
    "        #            padding=padding_value)\n",
    "\n",
    "        self.t_conv1 = nn.ConvTranspose2d(\n",
    "            latent_rep, hidden, kernel_size=(n_kernels_dec, n_kernels_dec), stride=stride_value, padding=padding_value)\n",
    "        # self.t_conv2 = nn.ConvTranspose2d(\n",
    "        #     hidden, latent_rep, kernel_size=(n_kernels_dec, n_kernels_dec), stride=stride_value, padding=padding_value)\n",
    "        self.t_conv2 = nn.ConvTranspose2d(\n",
    "            hidden, inout_channels, kernel_size=(3, 5), stride=(3, 5), padding=0)\n",
    "        print('')\n",
    "\n",
    "    def encoder(self, x):\n",
    "        #print(\"input: \" + str(x.shape))\n",
    "        z = F.relu(self.batchNorm1(self.conv1(x)))\n",
    "        #print(\"conv1: \" + str(z.shape))\n",
    "        z = self.pool(F.relu(self.batchNorm2(self.conv2(z))))\n",
    "        #print(\"conv2: \" + str(z.shape))\n",
    "        # z = F.relu(self.conv3(z))\n",
    "        return z\n",
    "\n",
    "    def decoder(self, x):\n",
    "        z = F.relu(self.t_conv1(x))\n",
    "        #print (\"t_conv1: \" + str(z.shape))\n",
    "        z = torch.tanh(self.t_conv2(z))\n",
    "        #print(\"t_conv2: \" + str(z.shape))\n",
    "        # z = (self.t_conv3(z))\n",
    "        #print(\"t_conv3: \" + str(z.shape))\n",
    "        # z = t.sigmoid(self.t_conv3(z))\n",
    "        return z\n",
    "    \n",
    "    def forward(self, input):\n",
    "        latent_representation = self.encoder(input)\n",
    "        output = self.decoder(latent_representation)\n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "AE = Autoencoder1(inout_channels=params.inout_channels, hidden=params.hidden, n_kernels_enc=params.n_kernels_enc,\n",
    "                     n_kernels_dec=params.n_kernels_dec, latent_rep=params.latent_rep, stride_value=params.stride_value,\n",
    "                     padding_value=params.padding_value)  # initializing the autoencoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Autoencoder1(\n",
       "  (conv1): Conv2d(1, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "  (conv2): Conv2d(16, 4, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "  (pool): MaxPool2d(kernel_size=(3, 5), stride=(3, 5), padding=0, dilation=1, ceil_mode=False)\n",
       "  (batchNorm1): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (batchNorm2): BatchNorm2d(4, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (t_conv1): ConvTranspose2d(4, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "  (t_conv2): ConvTranspose2d(16, 1, kernel_size=(3, 5), stride=(3, 5))\n",
       ")"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "AE.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "optimizer = torch.optim.Adam(params=AE.parameters(), lr=0.001)\n",
    "criteria_AE = torch.nn.MSELoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "pycharm": {
     "is_executing": true,
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "modelfine epoch [1/5], loss:1.0229\n",
      "modelfine epoch [2/5], loss:1.0010\n",
      "modelfine epoch [3/5], loss:0.8084\n",
      "modelfine epoch [4/5], loss:0.8907\n",
      "modelfine epoch [5/5], loss:0.7471\n",
      "modelfine epoch [6/5], loss:0.8624\n",
      "modelfine epoch [7/5], loss:0.7110\n",
      "modelfine epoch [8/5], loss:0.8112\n",
      "modelfine epoch [9/5], loss:0.7762\n",
      "modelfine epoch [10/5], loss:0.7859\n",
      "modelfine epoch [11/5], loss:0.5692\n",
      "modelfine epoch [12/5], loss:0.7571\n",
      "modelfine epoch [13/5], loss:0.6742\n",
      "modelfine epoch [14/5], loss:0.7142\n",
      "modelfine epoch [15/5], loss:0.6274\n",
      "modelfine epoch [16/5], loss:0.7894\n",
      "modelfine epoch [17/5], loss:0.6347\n",
      "modelfine epoch [18/5], loss:0.6619\n",
      "modelfine epoch [19/5], loss:0.7076\n",
      "modelfine epoch [20/5], loss:0.7351\n"
     ]
    }
   ],
   "source": [
    "AE.train()\n",
    "for i in range(20):\n",
    "    inner_optim.zero_grad()\n",
    "    for sensor_values, activity in client_loader[0]:\n",
    "        predicted_activity = AE(sensor_values.to(device).float())\n",
    "        loss = criteria_AE(predicted_activity.float(), sensor_values.to(device).float())\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "    print('modelfine epoch [{}/{}], loss:{:.4f}'.format(i + 1, params.inner_step_for_model, loss.data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.6987799470413458\n"
     ]
    }
   ],
   "source": [
    "with torch.no_grad():\n",
    "    AE.eval()\n",
    "    prvs_loss_for_AE = 0\n",
    "    for sensor_values, _ in eval_loader[0]:\n",
    "        predicted_sensor_values = AE(sensor_values.to(device).float())\n",
    "        prvs_loss_for_AE += criteria_AE(predicted_sensor_values.to(device),sensor_values.to(device).float()).item() * sensor_values.size(0)\n",
    "        # every time, the loss is 47 and the length is around 6680\n",
    "    prvs_loss_for_AE /= len(eval_loader[0].dataset)\n",
    "print(prvs_loss_for_AE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "__init__() got an unexpected keyword argument 'latent_rep'",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mTypeError\u001B[0m                                 Traceback (most recent call last)",
      "\u001B[1;32m<ipython-input-23-0c1c5f768650>\u001B[0m in \u001B[0;36m<module>\u001B[1;34m\u001B[0m\n\u001B[1;32m----> 1\u001B[1;33m model = BASEModel(latent_rep= 4 * 3 * 6, #params.width * params.window_size * params.latent_rep,\n\u001B[0m\u001B[0;32m      2\u001B[0m                       out_dim=params.outputdim, hidden_layer=params.model_hidden_layer)  # initilizing the base model\n",
      "\u001B[1;31mTypeError\u001B[0m: __init__() got an unexpected keyword argument 'latent_rep'"
     ]
    }
   ],
   "source": [
    "model = BASEModel(latent_rep= 4 * 3 * 6, #params.width * params.window_size * params.latent_rep,\n",
    "                      out_dim=params.outputdim, hidden_layer=params.model_hidden_layer)  # initilizing the base model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": true,
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": true,
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "inner_optim_base = torch.optim.Adam(model.parameters(), lr=params.inner_lr, weight_decay=params.inner_wd)\n",
    "for i in range(5):\n",
    "    inner_optim_base.zero_grad()\n",
    "    for sensor_values, activity in client_labeled_loaders[0]:\n",
    "        encoded_sensor_values = AE.encoder(sensor_values.to(device).float())\n",
    "        predicted_activity = model(encoded_sensor_values)\n",
    "        loss = criteria_model(predicted_activity, activity.to(device))\n",
    "        loss.backward()\n",
    "        inner_optim.step()\n",
    "    print('modelfine epoch [{}/{}], loss:{:.4f}'.format(i + 1, params.inner_step_for_model, loss.data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with torch.no_grad():\n",
    "    model.eval()\n",
    "    prvs_loss_server_model = 0\n",
    "    f1_score_server = 0\n",
    "    for sensor_values, activity in eval_loader[0]:\n",
    "        encoded_sensor_values = AE.encoder(sensor_values.to(device).float())\n",
    "        predicted_activity = model(encoded_sensor_values.to(device).float())\n",
    "        prvs_loss_server_model += criteria_model(predicted_activity, activity.to(device)).item() * sensor_values.size(0)\n",
    "        f1_score_server += f1_loss(activity, predicted_activity) * sensor_values.size(0)\n",
    "    prvs_loss_server_model /= len(eval_loader[0].dataset)\n",
    "    f1_score_server /= len(eval_loader[0].dataset)\n",
    "\n",
    "print(\"loss: \" + str(prvs_loss_server_model))\n",
    "print(\"f1: \" + str(f1_score_server))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "plt.plot(sensor_values[5,0,8,:].cpu())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}